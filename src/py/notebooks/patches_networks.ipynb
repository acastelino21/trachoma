{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/lumargot/trachoma/src/py')\n",
    "\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # put -1 to not use any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lumargot/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[neptune] [warning] NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "[neptune] [warning] NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "from loaders.tt_dataset import TTDataModulePatch, TrainTransformsFullSeg, EvalTransformsFullSeg\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from  PIL  import  Image\n",
    "import math\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision import ops\n",
    "import torchmetrics\n",
    "from PIL import Image\n",
    "import monai\n",
    "\n",
    "import lightning.pytorch as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_point = \"/CMF/data/lumargot/trachoma/B images one eye/\"\n",
    "\n",
    "df = pd.read_csv('/CMF/data/lumargot/trachoma/patches/csv/new/Pret_excluded_clean.csv')\n",
    "\n",
    "df_test = pd.read_csv('/CMF/data/lumargot/trachoma/patches/csv/new/Pret_excluded_clean_fold0_test.csv')\n",
    "df_val = pd.read_csv('/CMF/data/lumargot/trachoma/patches/csv/new/Pret_excluded_clean_fold0_train_test.csv')    \n",
    "df_train = pd.read_csv('/CMF/data/lumargot/trachoma/patches/csv/new/Pret_excluded_clean_fold0_train_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 2, 3, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             label  class\n",
      "12             ECA      1\n",
      "16       Entropion      2\n",
      "19             Gap      3\n",
      "24  overcorrection      4\n",
      "70  Short Incision      5\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[~df['label'].isin(['Healthy', 'Reject'])]\n",
    "\n",
    "print(df[['label', 'class']].drop_duplicates())\n",
    "concat_eca = ['ECA', 'Gap','overcorrection']\n",
    "concat_ptt = ['Entropion','Short Incision']\n",
    "\n",
    "eca_val = df.loc[ df['label'] == concat_eca[0]]['class'].unique()\n",
    "ptt_val = df.loc[ df['label'] == concat_ptt[0]]['class'].unique()\n",
    "\n",
    "\n",
    "df.loc[ df['label'].isin(concat_eca), \"class\" ] = eca_val[0]\n",
    "df.loc[ df['label'].isin(concat_ptt), \"class\" ] = ptt_val[0]\n",
    "\n",
    "unique_classes = sorted(df['class'].unique())\n",
    "class_mapping = {value: idx for idx, value in enumerate(unique_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict = 0\n",
    "subjects_id = df['filename'].unique()\n",
    "for subject in subjects_id:\n",
    "\n",
    "  df_sub = df.loc[ df['filename'] == subject]\n",
    "\n",
    "  df_labels = df_sub['class'].unique()\n",
    "\n",
    "  if len(df_labels)> 1:\n",
    "    conflict += 1\n",
    "    # print(df_sub['class'].value_counts(normalize=True))\n",
    "\n",
    "ratio = 100* conflict/len(subjects_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total number of conflict = 137 -> 13.7% of dataset\n",
    "\n",
    "with a distribution between [50, 60[ = 39           -> 3.9%\n",
    "with a distribution between [60, 70[ = 44 + 39 = 85 -> 8.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    " \n",
    "    def forward(self, input_seq):\n",
    "        assert len(input_seq.size()) > 2\n",
    " \n",
    "        # reshape input data --> (samples * timesteps, input_size)\n",
    "        # squash timesteps\n",
    "\n",
    "        size = input_seq.size()\n",
    "\n",
    "        batch_size = size[0]\n",
    "        time_steps = size[1]\n",
    "\n",
    "        size_reshape = [batch_size*time_steps] + list(size[2:])\n",
    "        reshaped_input = input_seq.contiguous().view(size_reshape)\n",
    " \n",
    "        output = self.module(reshaped_input)\n",
    "        \n",
    "        output_size = output.size()\n",
    "        output_size = [batch_size, time_steps] + list(output_size[1:])\n",
    "        output = output.contiguous().view(output_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class EfficientNetV2SYOLTPatchv2(pl.LightningModule):\n",
    "    def __init__(self, num_classes,**kwargs):\n",
    "        super(EfficientNetV2SYOLTPatchv2, self).__init__()        \n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        class_weights = None\n",
    "        if hasattr(self.hparams, \"class_weights\"):\n",
    "            class_weights = torch.tensor(self.hparams.class_weights).to(torch.float32)\n",
    "            \n",
    "        self.loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=self.num_classes)\n",
    "\n",
    "\n",
    "        model_feat = nn.Sequential(\n",
    "            models.efficientnet_v2_s(pretrained=True).features,\n",
    "            ops.Conv2dNormActivation(1280, 1536),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(start_dim=1)\n",
    "            )\n",
    "        self.F = TimeDistributed(model_feat)\n",
    "        \n",
    "        self.V = nn.Linear(in_features=1536, out_features=64)\n",
    "\n",
    "        #####  multihead attention ####\n",
    "        self.A = nn.MultiheadAttention(embed_dim=1024, num_heads=4, batch_first=True)\n",
    "        self.V2A = nn.Linear(64, 1024)\n",
    "        \n",
    "        self.P = nn.Linear(in_features=1024, out_features=self.num_classes)        \n",
    "\n",
    "\n",
    "        # self.train_transform = transforms.Compose(\n",
    "        #     [\n",
    "        #         RandomRotate(degrees=90, keys=[\"img\", \"seg\"], interpolation=[transforms.functional.InterpolationMode.NEAREST, transforms.functional.InterpolationMode.NEAREST], prob=0.5), \n",
    "        #         RandomFlip(keys=[\"img\", \"seg\"], prob=0.5)\n",
    "        #     ]\n",
    "        # )\n",
    "\n",
    "    def set_feat_model(self, model_feat):\n",
    "        self.F.module = model_feat\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self, X_patches):\n",
    "\n",
    "        # x_bb = torch.stack([self.compute_bb(seg, pad=0.01) for seg in X[\"seg\"]])\n",
    "        # X_padded = torch.stack([self.pad_image_to_fixed_size(img, bb) for img, bb in zip(X[\"img\"], x_bb)])\n",
    "\n",
    "        # X_patches = [self.extract_patches(img_padded) for img_padded in X_padded]\n",
    "        # X_patches = torch.stack(X_patches)\n",
    "\n",
    "        x_f = self.F(X_patches)\n",
    "        x_v = self.V(x_f)\n",
    "\n",
    "        ##### Multihead Attention #####\n",
    "        x_v2a = self.V2A(x_v)\n",
    "        x_a, x_a_weights = self.A(x_v2a, x_v2a, x_v2a)  # Shape [BS, n_patches^2, 1024],[BS, n_patches^2, n_patches^2]\n",
    "\n",
    "        # use the weights to update x_a\n",
    "        sum_weights = torch.sum(x_a_weights, 1, keepdim=True)\n",
    "        attention_weights = x_a_weights / sum_weights   # Shape: [BS, n_patches^2, n_patches^2]\n",
    "\n",
    "        ##  mat1 (b×n×m), mat2 (b×m×p), out is (b×n×p)\n",
    "        x_a = x_a.transpose(1, 2)  # Shape: [BS, 1024, n_patches^2]\n",
    "        x_a = torch.bmm(x_a, attention_weights)  # Shape: [BS, 1024, n_patches^2]\n",
    "        x_a = x_a.transpose(1, 2)\n",
    "\n",
    "        x = self.P(x_a)\n",
    "        return x, X_patches, x_a, x_v,\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "\n",
    "        imgs, labels = train_batch['patches'], train_batch['labels']\n",
    "        # batch = self.train_transform(imgs)\n",
    "\n",
    "        x, X_patches, x_a, x_v, = self(imgs)\n",
    "        x = x.reshape(-1,self.num_classes)\n",
    "\n",
    "        loss = self.loss(x, labels.reshape(-1))\n",
    "        self.log('train_loss', loss, sync_dist=True)\n",
    "\n",
    "        self.accuracy(torch.argmax(x, dim=1), labels.reshape(-1))\n",
    "        self.log(\"train_acc\", self.accuracy, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "\n",
    "        imgs, labels = val_batch['patches'], val_batch['labels']\n",
    "        # batch = self.train_transform(imgs)\n",
    "\n",
    "        x, X_patches, x_a, x_v, = self(imgs)\n",
    "        x = x.reshape(-1,self.num_classes)\n",
    "\n",
    "        loss = self.loss(x, labels.reshape(-1))\n",
    "        self.log('val_loss', loss, sync_dist=True)\n",
    "\n",
    "        self.accuracy(torch.argmax(x, dim=1), labels.reshape(-1))\n",
    "        self.log(\"val_acc\", self.accuracy, sync_dist=True)\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        imgs, labels = test_batch['patches'], test_batch['labels']\n",
    "        # batch = self.train_transform(imgs)\n",
    "\n",
    "        x, X_patches, x_a, x_v, = self(imgs)\n",
    "        x = x.reshape(-1,self.num_classes)\n",
    "\n",
    "        out = [ torch.argmax(x, dim=1), labels.reshape(-1) ]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = TrainTransformsFullSeg()\n",
    "eval_transform = EvalTransformsFullSeg()\n",
    "\n",
    "ttdata = TTDataModulePatch(df_train, df_val, df_test, \n",
    "                         batch_size=2, num_workers=4, \n",
    "                         img_column='filename', \n",
    "                         mount_point=mount_point, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttdata.setup()\n",
    "dataload = ttdata.train_dataloader()\n",
    "ds = ttdata.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12207 [00:00<?, ?it/s]The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  0%|          | 0/12207 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(tqdm(dataload)):\n",
    "  patches, labels, img = batch['patches'].cuda(), batch['labels'].cuda(), batch['img'].cuda()\n",
    "  \n",
    "  data = {\"patches\": patches, \"labels\": labels, \"img\":img}\n",
    "  \n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNetV2SYOLTPatchv2(num_classes=7).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test_step(data, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flyby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
